{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_experimentation_notebook.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uij3Hou-pJ3V"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0U02iqCJftI"
      },
      "source": [
        "# Get helper functions\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "!wget https://raw.githubusercontent.com/daoterog/Solid_Domestic_Waste_Image_Classification/main/helper_functions/file_management.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjAD6l6XpQmM"
      },
      "source": [
        "# Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Helper functions\n",
        "from helper_functions import (create_tensorboard_callback, plot_loss_curves, \n",
        "                              unzip_data, compare_historys, walk_through_dir,\n",
        "                              make_confusion_matrix)\n",
        "from file_management import bring_data, split_images\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTzg4GdKpa_3"
      },
      "source": [
        "# Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtqW_hHprxC5"
      },
      "source": [
        "path_list = ['/content/drive/MyDrive/PI2/data/cardboard.zip',\n",
        "            '/content/drive/MyDrive/PI2/data/metal.zip']\n",
        "\n",
        "# Unzip the data\n",
        "bring_data(path_list=path_list)\n",
        "\n",
        "# Split the dataset into train and test subsets\n",
        "split_images(train_size=0.7, test_proportion=0.5)\n",
        "\n",
        "# How many images/classes are there?\n",
        "walk_through_dir(\"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiDMM6kU0w-V"
      },
      "source": [
        "# Image paths\n",
        "train_dir = '/content/data/train'\n",
        "validation_dir = '/content/data/validation'\n",
        "test_dir =  '/content/data/test'\n",
        "\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Load in the data\n",
        "train_data = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
        "                                                         label_mode='categorical',\n",
        "                                                         image_size=IMG_SIZE,\n",
        "                                                         batch_size=BATCH_SIZE,\n",
        "                                                         shuffle=False)\n",
        "\n",
        "validation_data = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
        "                                                         label_mode='categorical',\n",
        "                                                         image_size=IMG_SIZE,\n",
        "                                                         batch_size=BATCH_SIZE,\n",
        "                                                         shuffle=False)\n",
        "\n",
        "test_data = tf.keras.utils.image_dataset_from_directory(test_dir,\n",
        "                                                         label_mode='categorical',\n",
        "                                                         image_size=IMG_SIZE,\n",
        "                                                         batch_size=BATCH_SIZE,\n",
        "                                                         shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCAEoBXHMms8"
      },
      "source": [
        "# Define Early Stopping Callback\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukZ1d56WMqd1"
      },
      "source": [
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                 patience=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXq67H4lHgI5"
      },
      "source": [
        "# CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzgJz_1kD4zq"
      },
      "source": [
        "# Create a Data Augmentation Layer\n",
        "data_augmentation = Sequential([\n",
        "    # preprocessing.RandomCrop(height=0.2,width=0.3), # Don't works for some reason\n",
        "    preprocessing.RandomFlip(mode='horizontal'),\n",
        "    preprocessing.RandomFlip(mode='vertical'),\n",
        "    # preprocessing.RandomHeight(0.2),\n",
        "    preprocessing.RandomRotation(0.3),\n",
        "    preprocessing.RandomTranslation(0.3,0.3),\n",
        "    # preprocessing.RandomWidth(0.2),\n",
        "    # preprocessing.RandomZoom(0.2),\n",
        "    preprocessing.RandomContrast(0.3),\n",
        "    # layers.Rescaling(255.),\n",
        "    layers.Resizing(224,224)\n",
        "], name='data_augmentation_layer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS0-vSjMg6sD"
      },
      "source": [
        "cnn_data_augmentation = Sequential([\n",
        "    data_augmentation,\n",
        "    layers.Conv2D(32, 4, activation='relu', padding='same', \n",
        "                  kernel_regularizer=regularizers.l2(0.05)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Conv2D(20, 3, activation='relu', padding='same', \n",
        "                  kernel_regularizer=regularizers.l2(0.05)),\n",
        "    layers.Conv2D(15, 3, activation='relu', padding='same', \n",
        "                  kernel_regularizer=regularizers.l2(0.05)),\n",
        "    layers.Conv2D(12, 3, activation='relu', padding='same', \n",
        "                  kernel_regularizer=regularizers.l2(0.05)),\n",
        "    layers.MaxPool2D(),\n",
        "    layers.Conv2D(10, 3, activation='relu', padding='same', \n",
        "                  kernel_regularizer=regularizers.l2(0.05)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(2, activation='softmax', name='output_layer')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_data_augmentation.compile(loss='binary_crossentropy',\n",
        "                            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "cnn_data_augmentation_history = cnn_data_augmentation.fit(train_data,\n",
        "                                                          epochs=50,\n",
        "                                                          steps_per_epoch=len(train_data),\n",
        "                                                          validation_data=validation_data,\n",
        "                                                          batch_size=BATCH_SIZE*8,\n",
        "                                                          validation_steps=len(validation_data),\n",
        "                                                          callbacks=[create_tensorboard_callback(\n",
        "                                                              dir_name='research_practice',\n",
        "                                                              experiment_name='CNN'\n",
        "                                                          ), earlystopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hee4kfmPN2UH"
      },
      "source": [
        "# Assemble model\n",
        "cnn_data_augmentation.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndIdkbdFoHu6"
      },
      "source": [
        "# Evaluate Model\n",
        "cnn_data_augmentation_results = cnn_data_augmentation.evaluate(test_data)\n",
        "cnn_data_augmentation_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWDtoytfoMx3"
      },
      "source": [
        "plot_loss_curves(cnn_data_augmentation_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328lnnpN4wKb"
      },
      "source": [
        "# Feature Extraction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59XWdM4n5zUV"
      },
      "source": [
        "# Create Feature Extraction Model\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Define inputs\n",
        "inputs = layers.Input(shape=IMG_SIZE + (3,), name='input_layer')\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "outputs = layers.Dense(len(train_data.class_names), activation='softmax', \n",
        "                       name='output_layer')(x)\n",
        "\n",
        "# Build Model\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Get summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGq17_MP7Uwc"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Fit it\n",
        "model_feature_vector_data_aug_history = model.fit(\n",
        "    train_data, epochs=50, validation_data=validation_data, \n",
        "    callbacks=[earlystopping, create_tensorboard_callback(\n",
        "        dir_name='research_practice',\n",
        "        experiment_name='feature_extraction_model'\n",
        "    )]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhDs-BFtHqlw"
      },
      "source": [
        "# Evaluate Model\n",
        "results_model_feature_vector_data_aug = model.evaluate(test_data)\n",
        "results_model_feature_vector_data_aug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fBZ6ag-KdDK"
      },
      "source": [
        "plot_loss_curves(model_feature_vector_data_aug_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOosy8aLK2cl"
      },
      "source": [
        "# Fine-Tuning Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdK9DDf6LGtL"
      },
      "source": [
        "# Let's unfreeze some layers of the base_model\n",
        "NUM_UNFROZEN_LAYERS = 5\n",
        "\n",
        "base_model.trainale = True\n",
        "for layer in model.layers[2].layers[:-NUM_UNFROZEN_LAYERS]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVJqUNWXLw1f"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Fit it\n",
        "fine_tuning_epochs = 10\n",
        "\n",
        "model_fine_tuned_history = model.fit(\n",
        "    train_data, epochs=50, validation_data=validation_data, \n",
        "    initial_epoch=model_feature_vector_data_aug_history.epoch[-1],\n",
        "    callbacks=[earlystopping, create_tensorboard_callback(\n",
        "        dir_name='research_practice',\n",
        "        experiment_name='fine_tuning_model'\n",
        "    )]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np8fJofVL_ig"
      },
      "source": [
        "# Evaluate the fine-tuned model\n",
        "results_model_fine_tuned = model.evaluate(test_data)\n",
        "results_model_fine_tuned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOijV26LNO-j"
      },
      "source": [
        "# Compare history\n",
        "compare_historys(original_history=model_feature_vector_data_aug_history,\n",
        "                new_history=model_fine_tuned_history,\n",
        "                initial_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LJVLHsuzoyP"
      },
      "source": [
        "# Upload to TensorBoard Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlX1JyrmzsUs"
      },
      "source": [
        "!tensorboard dev list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3BJHEDXzzng"
      },
      "source": [
        "!tensorboard dev upload --logdir /content/research_practice --name \"Research Practice Model Experiments\" --one_shot --description \"Here are stored the results of the final models resulting from my research practice II. You can check the notebook where this was generated in https://github.com/daoterog/Solid_Domestic_Waste_Image_Classification\" \\\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZo3xZyRNz0k"
      },
      "source": [
        "# Evaluating the performance across classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwk8n_TauC79"
      },
      "source": [
        "# Make prediction with the model\n",
        "pred_probs = cnn_data_augmentation.predict(test_data, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrZDtp88Ohnm"
      },
      "source": [
        "# Make prediction with the model\n",
        "pred_probs = model.predict(test_data, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjHfU-nqPvWw"
      },
      "source": [
        "# Get image labels\n",
        "y_labels = []\n",
        "for _, label in test_data.unbatch():\n",
        "    y_labels.append(label.numpy().argmax())\n",
        "\n",
        "# Get predicted labels\n",
        "y_pred = pred_probs.argmax(axis = 1)\n",
        "\n",
        "# Get class names\n",
        "class_names = test_data.class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPp5XoQkPUQJ"
      },
      "source": [
        "# Make confusion matrix with predicted labels\n",
        "make_confusion_matrix(y_true=y_labels,\n",
        "                      y_pred=y_pred,\n",
        "                      classes=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNK-pGYCQ1WN"
      },
      "source": [
        "classification_report_dict = classification_report(y_true=y_labels,\n",
        "                                                   y_pred=y_pred,\n",
        "                                                   output_dict=True)\n",
        "\n",
        "# Create empty dictionary\n",
        "class_f1_scores = {}\n",
        "\n",
        "# Loop through the classification report items\n",
        "for key, value in classification_report_dict.items():\n",
        "    if key == 'accuracy':\n",
        "        break\n",
        "    else:\n",
        "        class_f1_scores[class_names[int(key)]] = value['f1-score']\n",
        "\n",
        "class_f1_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQLCHCHlSNR7"
      },
      "source": [
        "f1_scores = pd.DataFrame({\"class_name\": list(class_f1_scores.keys()),\n",
        "                          \"f1-score\": list(class_f1_scores.values())})\\\n",
        "                          .sort_values(\"f1-score\", ascending=False)\n",
        "\n",
        "f1_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUNS82Z_Sl5s"
      },
      "source": [
        "def f1_scores_barplot(f1_scores, figsize=(10, 10)):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    scores = ax.barh(range(len(f1_scores)), f1_scores[\"f1-score\"].values)\n",
        "    ax.set_yticks(range(len(f1_scores)))\n",
        "    ax.set_yticklabels(list(f1_scores[\"class_name\"]))\n",
        "    ax.set_xlabel(\"f1-score\")\n",
        "    ax.set_title(\"F1-Scores for each Class\")\n",
        "    ax.invert_yaxis(); # reverse the order\n",
        "\n",
        "f1_scores_barplot(f1_scores, figsize=(5,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCv0pdhkTCS1"
      },
      "source": [
        "# Find the most wrong predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1WXoD99TqWX"
      },
      "source": [
        "# Get the filenames of our test data\n",
        "filepaths = []\n",
        "for filepath in test_data.list_files('/content/data/test/*/*.jpg',\n",
        "                                     shuffle=False):\n",
        "    filepaths.append(filepath.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2utfQ6QUmni"
      },
      "source": [
        "# Create DataFrame\n",
        "pred_df = pd.DataFrame({\n",
        "    'img_path': filepaths,\n",
        "    'y_true': y_labels,\n",
        "    'y_pred': y_pred,\n",
        "    'pred_prob': pred_probs.max(axis=1),\n",
        "    'y_true_classname': [class_names[y] for y in y_labels],\n",
        "    'y_pred_classname': [class_names[y] for y in y_pred]\n",
        "})\n",
        "\n",
        "# Add column that indicates wether the prediction was right\n",
        "pred_df['pred_correct'] = pred_df.y_true == pred_df.y_pred\n",
        "\n",
        "pred_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7yPgTPBVtQp"
      },
      "source": [
        "# Get wrong predictions and sort them by their probability\n",
        "wrong_preds = pred_df[~pred_df.pred_correct].sort_values(by='pred_prob',\n",
        "                                                            ascending=False)\n",
        "wrong_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRqPkVWXXUmr"
      },
      "source": [
        "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
        "    \"\"\"\n",
        "    Reads in an image from filename, turns it into a tensor and reshapes into\n",
        "    (224, 224, 3).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename (str): string filename of target image\n",
        "    img_shape (int): size to resize target image to, default 224\n",
        "    scale (bool): whether to scale pixel values to range(0, 1), default True\n",
        "    \"\"\"\n",
        "    # Read in the image\n",
        "    img = tf.io.read_file(filename)\n",
        "    # Decode it into a tensor\n",
        "    img = tf.io.decode_image(img)\n",
        "    # Resize the image\n",
        "    img = tf.image.resize(img, [img_shape, img_shape])\n",
        "    if scale:\n",
        "        # Rescale the image (get all values between 0 and 1)\n",
        "        return img/255.\n",
        "    else:\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZvK_BzZWOvd"
      },
      "source": [
        "for row in wrong_preds.itertuples():\n",
        "    _, img_path, _, _, pred_prob, true_cn, pred_cn, _ = row\n",
        "\n",
        "    img = load_and_prep_image(img_path, scale=True)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"actual: {true_cn}, pred: {pred_cn} \\nprob: {pred_prob:.2f}\")\n",
        "    plt.axis(False)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40pDa-_CW5KL"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "roc_auc_score(y_labels,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFxBy1eoxnwx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}